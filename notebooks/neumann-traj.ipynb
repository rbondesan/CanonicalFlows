{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test finding conserved quantities in Neumann model - traj2circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'neumann_hamiltonian'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-586c475e7fad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhamiltonians\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mneumann_hamiltonian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_train_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'neumann_hamiltonian'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "from models import *\n",
    "from hamiltonians import neumann_hamiltonian\n",
    "from utils import make_train_op\n",
    "from losses import make_loss\n",
    "from data import make_data\n",
    "from utils import visualize_chain_bijector, is_symplectic\n",
    "\n",
    "DTYPE=tf.float32\n",
    "NP_DTYPE=np.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = [.1, .2, .3]\n",
    "settings = {\n",
    "    'frequencies': frequencies,\n",
    "    'hamiltonian': parameterized_neumann(frequencies),\n",
    "    'd': 3,                    # space dimension\n",
    "    'num_particles': 1,        # number of particles\n",
    "    'minibatch_size': 2**7,    # Mini batch size\n",
    "    'dataset_size': 2**13,      # Set to float(\"inf\") for keeping sampling.\n",
    "    'num_stacks_bijectors': 4, # Number of bijectors\n",
    "    'log_dir' : \"/tmp/log/im_tests/neumann-traj/\",\n",
    "    'ckpt_freq': 1000,\n",
    "    'train_iters': 2,\n",
    "    'visualize': True,\n",
    "#    'grad_clip_norm': 1e-10, # clip norm to val. Comment for no gradient clipping\n",
    "    'starter_learning_rate': 0.00001,    \n",
    "    'decay_lr': \"piecewise\",\n",
    "    'boundaries': [20000, 100000, 200000], # for piecewise decay\n",
    "    'values': [1e-3, 1e-4, 1e-5, 1e-6],  # for piecewise decay\n",
    "    'min_learning_rate': 1e-6   \n",
    "#     'decay_steps': 25000,  # ignored if decay_lr False\n",
    "#     'decay_rate': 0.5,     # ignored if decay_lr False (decayed_learning_rate = learning_rate *\n",
    "#                            #                            decay_rate ^ (global_step / decay_steps))\n",
    "#    'loss': \"dKdphi\",\n",
    "#    'base_dist': \"action_dirac_angle\",\n",
    "#    'value_actions': [0.1324, 0.0312, 0.2925],\n",
    "#    'elastic_net_coeff': 1.\n",
    "    }\n",
    "# Choose a batch of actions: needs to be divisor of dataset_size or minibatch_size if infinite dataset\n",
    "# r = np.random.RandomState(seed=0)\n",
    "# num_samples_actions = 2 # number of distinct actions (Liouville torii)\n",
    "# sh = (num_samples_actions, settings['d'], settings['num_particles'], 1)\n",
    "# settings['value_actions'] = r.rand(*sh).astype(NP_DTYPE)\n",
    "# print(settings['value_actions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 1\n",
    "x0 = np.reshape([1,.1,2,.2,3,.3], (1, settings['d'], settings['num_particles'], 2)).astype(NP_DTYPE)\n",
    "q0,p0 = extract_q_p(x0)\n",
    "print(\"q0 = \", np.reshape(q0,[-1]))\n",
    "print(\"p0 = \", np.reshape(p0,[-1]))\n",
    "print(\"E  = \", sess.run( settings['hamiltonian'](tf.constant(x0)) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand(6) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.random.randn(1, settings['d'], settings['num_particles'], 2).astype(NP_DTYPE)\n",
    "\n",
    "# Use HamiltonianFlow as integrator of Hamiltonian\n",
    "integrator = HamiltonianFlow(settings['hamiltonian'],\n",
    "                             initial_t=0., \n",
    "                             final_t=10., \n",
    "                             num_steps=settings['minibatch_size'])\n",
    "traj = integrator(x0, return_full_state=True)\n",
    "# traj has shape (num_time_samples,batch,d,n,2). Reinterpret batch and num_time_samples as batch\n",
    "print(traj.shape)\n",
    "#traj = np.reshape(traj, [-1,settings['d'],settings['num_particles'],2])\n",
    "qtraj, ptraj = sess.run(extract_q_p( tf.reshape(traj, [-1,settings['d'],settings['num_particles'],2]) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orbits\n",
    "cols=['r','g','b']\n",
    "for n in range(settings['d']):\n",
    "    plt.plot(qtraj[:,n,0,0], ptraj[:,n,0,0], cols[n]+'*')\n",
    "#plt.gca().set_aspect('equal', adjustable='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model and loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model T as sequence of symplectic transformations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No oscillator flow, map directly to phat,qhat cartesian coords of action-angle\n",
    "stack = []\n",
    "for i in range(settings['num_stacks_bijectors']):\n",
    "    stack.extend([ #ZeroCenter(is_training_forward=True),\n",
    "                  LinearSymplectic(),\n",
    "                  SymplecticAdditiveCoupling(shift_model=IrrotationalMLP())])\n",
    "                  #SymplecticAdditiveCoupling(shift_model=MLP())])\n",
    "T = Chain(stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data, loss and train op:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss imposes that the trajectories in the transformed coordinates $\\hat{q},\\hat{p}$ are on circles of radius given by the actions. We choose the following one which makes couples of points have the same radius ($s$ is the shift and $t$ runs along time steps):\n",
    "$$\n",
    "\\ell = \\sum_d \\sum_n \\sum_{t=0}^{T-1} \n",
    "\\left[\\hat{q}(t,d,n)^2+\\hat{p}(t,d,n)^2 - (\\hat{q}(t-s,d,n)^2+\\hat{p}(t-s,d,n)^2)\\right]^2\n",
    "$$\n",
    "\n",
    "We can also take random shifts each time the loss is computed, which can be implemented as shuffling the data with a permutation $\\pi$:\n",
    "$$\n",
    "\\ell = \\sum_d \\sum_n \\sum_{t=0}^{T-1} \n",
    "\\left[\\hat{q}(\\pi(t),d,n)^2+\\hat{p}(\\pi(t),d,n)^2 - (\\hat{q}(\\pi(t-s),d,n)^2+\\hat{p}(\\pi(t-s),d,n)^2)\\right]^2\n",
    "$$\n",
    "This can be achieved by shuffling data at each iteration using the data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_circle_loss(z, shift=-1):\n",
    "    zsq = tf.square(z)\n",
    "    qhatsq, phatsq = extract_q_p(zsq)    \n",
    "#     if settings['visualize']:\n",
    "#         tf.summary.histogram(\"qhatsq\", qhatsq)\n",
    "#         tf.summary.histogram(\"phatsq\", phatsq)\n",
    "\n",
    "    diff_qhatsq = qhatsq - tf.roll(qhatsq, shift=shift, axis=0)\n",
    "    diff_phatsq = phatsq - tf.roll(phatsq, shift=shift, axis=0)\n",
    "    \n",
    "#    return tf.reduce_mean(tf.square(diff_qhatsq + diff_phatsq))\n",
    "    return tf.reduce_sum(tf.square(diff_qhatsq + diff_phatsq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = tf.get_variable(\"global_step\", [], tf.int64, tf.zeros_initializer(), trainable=False)\n",
    "\n",
    "traj = tf.constant(traj, dtype=DTYPE)\n",
    "\n",
    "# shuffle data along time dim - 0th axis\n",
    "# TODO: use data loader\n",
    "traj = tf.random_shuffle(traj)\n",
    "\n",
    "from utils import extract_q_p\n",
    "with tf.name_scope(\"canonical_transformation\"):\n",
    "    # traj is (num_time_samples,batch,d,n,2)\n",
    "    num_time_samples = traj.shape[0]\n",
    "    batch = traj.shape[1]    \n",
    "    traj = tf.reshape(traj, [num_time_samples*batch,settings['d'],settings['num_particles'],2]) \n",
    "    z = T.inverse(traj)\n",
    "\n",
    "    loss = make_circle_loss(z,shift=-settings['minibatch_size']//2)\n",
    "    \n",
    "    tf.summary.scalar('loss', loss)\n",
    "    \n",
    "train_op = make_train_op(settings, loss, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_chain_bijector(T, traj, sess=sess, inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "# Coordinates to monitor\n",
    "dd = 0; nn = 0\n",
    "loss_np = 1 # Init\n",
    "loss_converged = 5e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(settings['log_dir'], sess.graph)\n",
    "checkpoint_dir = settings['log_dir']\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "saver = tf.train.Saver(max_to_keep=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while loss_np > loss_converged:\n",
    "    _, summary, it, loss_np = sess.run([train_op, merged, step, loss])\n",
    "    writer.add_summary(summary, it)\n",
    "    losses.append(loss_np)\n",
    "    \n",
    "    if it % settings['ckpt_freq'] == 0:\n",
    "        saver.save(sess, checkpoint_prefix, global_step=it) #, write_meta_graph=True)\n",
    "        \n",
    "    if it % 1000 == 0:        \n",
    "        display.clear_output(wait=True)\n",
    "        print(\"Loss at {} is {}\".format(it, loss_np))\n",
    "        fig = plt.figure(figsize=(12,4))\n",
    "        qhat_traj,phat_traj = sess.run(extract_q_p(z))\n",
    "\n",
    "        cols=['r','g','b']        \n",
    "        for n in range(settings['d']):\n",
    "            plt.subplot(1, settings['d'], n + 1)\n",
    "            plt.plot(qtraj[:,n,0,0], ptraj[:,n,0,0], cols[n]+'-')\n",
    "            plt.plot(qhat_traj[:,n,0,0], phat_traj[:,n,0,0], cols[n]+'*')\n",
    "        \n",
    "        plt.gca().set_aspect('equal', adjustable='box')\n",
    "        #plt.savefig('../results/kepler/image_at_it_{:04d}.png'.format(it))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "plt.plot(qtraj[:,n,0,0], ptraj[:,n,0,0], '-')\n",
    "plt.plot(qhat_traj[:,n,0,0], phat_traj[:,n,0,0], '*')\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.savefig('../results/neumann_t2c.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(losses[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_chain_bijector(T, traj, sess=sess, inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
