{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimize Hamiltonian 1d chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on shapes\n",
    "Tensors flowing in the network represent batches of states of a dynamical system. Similarly to what is done for images, audio or other signals in deep learning, we distinguish a physical from a hidden or internal dimension. In images, these are the spatial and channel dimension, where the latter encodes features represeting abstract concept of the input space. Therefore, tensors will have shape (batches, phase_space, internal). While the physical system has internal dimension = 1, we will use convolutional layers which treat differently the phase space from the internal or channel dimension, and so we separate these dimensions from the beginning and use convolutions to extract features characterizing the physical systems and which will increase the internal dimension. Here we study 1d, where phase_space dimension equals twice the number of particles.\n",
    "\n",
    "In particular, we take the following basis of R^2n: (q1,p1,q2,p2,...,qn,pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../hamiltonians.py:29: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\n",
      "../hamiltonians.py:37: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\n",
      "../hamiltonians.py:46: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\n",
      "../hamiltonians.py:55: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\n",
      "../hamiltonians.py:64: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\n",
      "../hamiltonians.py:73: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from hamiltonians import fpu_beta_hamiltonian\n",
    "from models import NICE, SymplecticExchange, SqueezeAndShift, Chain\n",
    "from models import MLP, MultiScaleArchitecture, CNNShiftModel, CNNShiftModel2\n",
    "from utils import visualize_chain_bijector_1d, compute_loss, compute_gradients, apply_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress the warning till they fix this:\n",
    "# lib/python3.5/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE=tf.float32\n",
    "NP_DTYPE=np.float32\n",
    "settings = {\n",
    "    'hamiltonian': fpu_beta_hamiltonian, \n",
    "    'num_scales' : 1,     # num scales architecture        \n",
    "    'phase_space_dim': 4, # Need phase_space_dim at least 2**num_scales. Also, = 2 * num_particles for chains.\n",
    "    'batch_size': 128,    # Mini batch size \n",
    "    'num_steps_flow': 4, # num bijector per step of flow\n",
    "    'train_iters': int(1e4)}   # Number of training iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dist = tfd.MultivariateNormalDiag(loc=tf.zeros([settings['phase_space_dim']], DTYPE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create architecture for model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows = []\n",
    "for i in range(settings['num_scales']):\n",
    "    bijectors = [SqueezeAndShift(shift_model=CNNShiftModel2()) if i % 2 == 0 \n",
    "             else SymplecticExchange() \n",
    "             for i in range(settings['num_steps_flow'])]\n",
    "    flows.append(Chain(bijectors))\n",
    "model = MultiScaleArchitecture(flows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 11.619413\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "\n",
    "global_step = []\n",
    "losses = []\n",
    "for epoch in range(settings['train_iters']):\n",
    "    z_samples = tf.expand_dims(base_dist.sample(settings['batch_size']),-1)\n",
    "    gradients, loss = compute_gradients(model, settings['hamiltonian'], z_samples)\n",
    "    apply_gradients(optimizer, gradients, model.trainable_variables)\n",
    "    # Visualization\n",
    "    if epoch % 1e2 == 0:\n",
    "        global_step.append(epoch)\n",
    "        losses.append(loss.numpy())\n",
    "    if epoch % int(1e3) == 0:\n",
    "        print(epoch, loss.numpy())\n",
    "end_time = time.time()\n",
    "print(\"Time elapsed: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "plt.plot(losses[start:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save:\n",
    "from utils import checkpoint_save\n",
    "checkpoint_save(settings, optimizer, model,\n",
    "                tf.train.get_or_create_global_step())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore:\n",
    "# checkpoint_restore(settings, optimizer, model, optimizer_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy\n",
    "z_samples = tf.expand_dims(base_dist.sample(settings['batch_size']),-1)\n",
    "test_error = compute_loss(model, settings['hamiltonian'], z_samples)\n",
    "print(\"Test error: \", test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test symmetries\n",
    "Lattice shift means that the learnt prob distribution is invariant under shifts. Symplecticity, check relation with symplectic form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test lattice shift symmetry\n",
    "from utils import lattice_shift\n",
    "\n",
    "x = tf.random_normal((1,settings['phase_space_dim'],1), dtype=DTYPE) # q_1, p_1, q_2, p_2\n",
    "z = tf.reshape(model.inverse(x), shape=[settings['phase_space_dim']])\n",
    "x_shifted = lattice_shift(x) # q_2, p_2, q_1, p_1\n",
    "z_shifted = tf.reshape(model.inverse(x_shifted), shape=[settings['phase_space_dim']])\n",
    "print(base_dist.log_prob(z) - base_dist.log_prob(z_shifted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test symplecticity\n",
    "from utils import is_symplectic\n",
    "\n",
    "x = tf.random_normal((1,settings['phase_space_dim'],1), dtype=DTYPE)\n",
    "is_symplectic(model, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is symplectic and the learnt distribution is lattice shift invariant as required for a chain. While the multi-scale architecture is by design symplectic as well, lattice invariance is harder to achieve in that case and longer training is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow of the system\n",
    "TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
